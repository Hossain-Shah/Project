{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TdECnsXmp6Su",
        "outputId": "3c2d8427-e160-4e40-895c-4e38412ef00f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-google-genai\n",
            "  Downloading langchain_google_genai-2.0.11-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.19-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting autopep8\n",
            "  Downloading autopep8-2.3.2-py2.py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: gitpython in /usr/local/lib/python3.11/dist-packages (3.1.44)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from langchain-google-genai)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting google-ai-generativelanguage<0.7.0,>=0.6.16 (from langchain-google-genai)\n",
            "  Downloading google_ai_generativelanguage-0.6.16-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.37 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (0.3.40)\n",
            "Requirement already satisfied: pydantic<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (2.10.6)\n",
            "Collecting langchain-core<0.4.0,>=0.3.37 (from langchain-google-genai)\n",
            "  Downloading langchain_core-0.3.43-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting langchain<1.0.0,>=0.3.20 (from langchain_community)\n",
            "  Downloading langchain-0.3.20-py3-none-any.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.38)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.11.13)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (9.0.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
            "  Downloading pydantic_settings-2.8.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.11)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain_community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy<3,>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (1.26.4)\n",
            "Collecting pycodestyle>=2.12.0 (from autopep8)\n",
            "  Downloading pycodestyle-2.12.1-py2.py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython) (4.0.12)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.18.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython) (5.0.2)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (2.24.1)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (1.26.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (4.25.6)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.20->langchain_community) (0.3.6)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (2.27.2)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2025.1.31)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.1.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (1.69.0)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (1.70.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (1.62.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (4.9)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (3.0.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (0.6.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.3.1)\n",
            "Downloading langchain_google_genai-2.0.11-py3-none-any.whl (39 kB)\n",
            "Downloading langchain_community-0.3.19-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading autopep8-2.3.2-py2.py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading google_ai_generativelanguage-0.6.16-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading langchain-0.3.20-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.43-py3-none-any.whl (415 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m415.4/415.4 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycodestyle-2.12.1-py2.py3-none-any.whl (31 kB)\n",
            "Downloading pydantic_settings-2.8.1-py3-none-any.whl (30 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: filetype, python-dotenv, pycodestyle, mypy-extensions, marshmallow, httpx-sse, typing-inspect, autopep8, pydantic-settings, dataclasses-json, langchain-core, google-ai-generativelanguage, langchain-google-genai, langchain, langchain_community\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.40\n",
            "    Uninstalling langchain-core-0.3.40:\n",
            "      Successfully uninstalled langchain-core-0.3.40\n",
            "  Attempting uninstall: google-ai-generativelanguage\n",
            "    Found existing installation: google-ai-generativelanguage 0.6.15\n",
            "    Uninstalling google-ai-generativelanguage-0.6.15:\n",
            "      Successfully uninstalled google-ai-generativelanguage-0.6.15\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.19\n",
            "    Uninstalling langchain-0.3.19:\n",
            "      Successfully uninstalled langchain-0.3.19\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-generativeai 0.8.4 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.16 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed autopep8-2.3.2 dataclasses-json-0.6.7 filetype-1.2.0 google-ai-generativelanguage-0.6.16 httpx-sse-0.4.0 langchain-0.3.20 langchain-core-0.3.43 langchain-google-genai-2.0.11 langchain_community-0.3.19 marshmallow-3.26.1 mypy-extensions-1.0.0 pycodestyle-2.12.1 pydantic-settings-2.8.1 python-dotenv-1.0.1 typing-inspect-0.9.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "ec6a0fb468e740a8a7710a7c3b6f6ece"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# installing dependencies\n",
        "!pip install langchain-google-genai langchain_community autopep8 gitpython"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary modules with error handling\n",
        "try:\n",
        "    from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
        "    from langchain_community.document_loaders import WebBaseLoader\n",
        "    from langchain.chains import StuffDocumentsChain\n",
        "    from langchain.chains.llm import LLMChain\n",
        "    from langchain.prompts import PromptTemplate\n",
        "    import google.generativeai as genai\n",
        "    import os\n",
        "    from google.colab import userdata\n",
        "    import git\n",
        "    import glob\n",
        "    import subprocess\n",
        "    import autopep8  # For Python code formatting\n",
        "    import shutil\n",
        "except ModuleNotFoundError as e:\n",
        "    print(f\"Module not found: {e}\")\n",
        "    # Install missing modules if necessary\n",
        "    !pip install langchain-google-genai langchain-community\n",
        "\n",
        "# Configure API key and initialize model\n",
        "if \"GOOGLE_API_KEY\" not in os.environ:\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = userdata.get('api_key')\n",
        "\n",
        "genai.configure(api_key=os.environ['GOOGLE_API_KEY'])\n",
        "\n",
        "# Function to initialize model\n",
        "def initialize_model():\n",
        "    try:\n",
        "        llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\")\n",
        "        return llm\n",
        "    except ValueError as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        print(\"Switching to alternative model.\")\n",
        "        # Check if alternative model is available, otherwise handle accordingly\n",
        "        try:\n",
        "            from langchain_alternative_module import ChatAlternativeModel  # Placeholder name\n",
        "            llm = ChatAlternativeModel(model=\"alternative-model\")  # Specify the correct model name if needed\n",
        "            return llm\n",
        "        except ModuleNotFoundError:\n",
        "            print(\"Alternative model module not found.\")\n",
        "            return None\n",
        "\n",
        "# Initialize the model\n",
        "llm = initialize_model()"
      ],
      "metadata": {
        "id": "fxUM94gtq3Qu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b3b812f-a416-4e84-9bd4-7528988644d0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone the repository (or use a local directory)\n",
        "REPO_URL = \"https://github.com/Hossain-Shah/Robi_Datathon_0100_pandas.git\"\n",
        "LOCAL_REPO_PATH = \"/content/drive/MyDrive/Colab_Notebook/Robi_Datathon_0100_pandas\"\n",
        "\n",
        "def clone_repo(repo_url, local_path):\n",
        "    \"\"\"Clones a GitHub repository if not already cloned.\"\"\"\n",
        "    if not os.path.exists(local_path):\n",
        "        print(f\"Cloning repository from {repo_url}...\")\n",
        "        git.Repo.clone_from(repo_url, local_path)\n",
        "    else:\n",
        "        print(\"Repository already cloned.\")\n",
        "\n",
        "def read_code_files(repo_path, extensions=[\".py\", \".ipynb\", \".js\", \".java\", \".cpp\", \".ts\"]):\n",
        "    \"\"\"Reads all code files from the repository.\"\"\"\n",
        "    files = []\n",
        "    for ext in extensions:\n",
        "        files.extend(glob.glob(f\"{repo_path}/**/*{ext}\", recursive=True))\n",
        "\n",
        "    code_contents = {}\n",
        "    for file in files:\n",
        "        with open(file, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "            code_contents[file] = f.read()\n",
        "    return code_contents\n",
        "\n",
        "def summarize_code(file_path, code):\n",
        "    \"\"\"Summarizes code using Google-gemini model.\"\"\"\n",
        "    prompt_template = PromptTemplate.from_template(\"Summarize the following code from {file_path}:\\n\\n{code}\\n\\nSummary:\")\n",
        "    prompt = prompt_template.format(file_path=file_path, code=code)\n",
        "    llm_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
        "    stuff_chain = StuffDocumentsChain(llm_chain=llm_chain, document_variable_name=\"code\")\n",
        "    response = llm_chain.run({\"file_path\": file_path, \"code\": code})\n",
        "    return response\n",
        "\n",
        "def analyze_repository():\n",
        "    \"\"\"Main function to analyze a repository.\"\"\"\n",
        "    clone_repo(REPO_URL, LOCAL_REPO_PATH)\n",
        "    code_files = read_code_files(LOCAL_REPO_PATH)\n",
        "\n",
        "    repo_summary = []\n",
        "    for file, code in code_files.items():\n",
        "        print(f\"Analyzing {file}...\")\n",
        "        summary = summarize_code(file, code[:2000])  # Limit input size\n",
        "        repo_summary.append(f\"📄 **{file}**:\\n{summary}\\n\")\n",
        "\n",
        "    # Generate final repository summary\n",
        "    final_summary_template = PromptTemplate.from_template(\"Provide an overview of the repository structure based on these file summaries:\\n\\n{repo_summaries}\")\n",
        "    final_prompt = final_summary_template.format(repo_summaries=\"\\n\".join(repo_summary))\n",
        "    llm_chain = LLMChain(llm=llm, prompt=final_summary_template)\n",
        "    final_summary = llm_chain.run({\"repo_summaries\": \"\\n\".join(repo_summary)})\n",
        "\n",
        "    # Print and save the summary\n",
        "    print(\"\\n📝 Repository Summary:\\n\", final_summary)\n",
        "    with open(\"/content/drive/MyDrive/Colab_Notebooks/repo_summary.txt\", \"w\") as f:\n",
        "        f.write(final_summary)\n",
        "\n",
        "# Run the analysis\n",
        "if __name__ == \"__main__\":\n",
        "    analyze_repository()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVz4FzbRtxP2",
        "outputId": "5b2a492d-14b7-42db-adfb-1d050b372386"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Repository already cloned.\n",
            "Analyzing /content/drive/MyDrive/Colab_Notebook/Robi_Datathon_0100_pandas/shahnawaz/utils/Robi_Datathon_problems_solution.ipynb...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-2ee4c312ae6a>:29: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
            "  llm_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
            "<ipython-input-3-2ee4c312ae6a>:30: LangChainDeprecationWarning: This class is deprecated. Use the `create_stuff_documents_chain` constructor instead. See migration guide here: https://python.langchain.com/docs/versions/migrating_chains/stuff_docs_chain/\n",
            "  stuff_chain = StuffDocumentsChain(llm_chain=llm_chain, document_variable_name=\"code\")\n",
            "<ipython-input-3-2ee4c312ae6a>:31: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  response = llm_chain.run({\"file_path\": file_path, \"code\": code})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📝 Repository Summary:\n",
            " The repository structure is relatively simple, containing a Jupyter Notebook within a nested directory structure reflecting its location on Google Drive.  All the data files used by the notebook are assumed to be in the same directory as the notebook itself.\n",
            "\n",
            "```\n",
            "Colab_Notebook/\n",
            "└── Robi_Datathon_0100_pandas/\n",
            "    └── shahnawaz/\n",
            "        └── utils/\n",
            "            └── Robi_Datathon_problems_solution.ipynb\n",
            "            └── purchase.csv  (Assumed location)\n",
            "            └── boxes.csv      (Assumed location)\n",
            "            └── problem1.csv   (Assumed location)\n",
            "            └── submission_1.csv (Output file generated by the notebook)\n",
            "            └── ... (Other data files for Problem 2 - names unknown)\n",
            "\n",
            "```\n",
            "\n",
            "The notebook `Robi_Datathon_problems_solution.ipynb` is the core component, containing the Python code for solving the data manipulation and prediction tasks.  It resides within a directory structure likely created through Google Colab and the user's Google Drive organization. The `shahnawaz/utils/` path suggests a user named \"shahnawaz\" and a directory dedicated to utility scripts or notebooks.\n",
            "\n",
            "The key data files (`purchase.csv`, `boxes.csv`, `problem1.csv`) are assumed to be located in the same directory as the notebook.  The notebook generates at least one output file, `submission_1.csv`, which also resides in the same directory. Other data files related to Problem 2 are expected to be present in the same location, although their names are not provided in the summary.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q&A System for Code Repository\n",
        "\n",
        "def answer_question_about_code(question, code_files):\n",
        "    \"\"\"Answers questions based on the code repository.\"\"\"\n",
        "    relevant_code = \"\\n\".join([f\"File: {file}\\n{code[:2000]}\" for file, code in code_files.items()])  # Limit input size\n",
        "    prompt_template = PromptTemplate.from_template(\"\"\"\n",
        "    You are an AI assistant analyzing a code repository. Answer the following question based on the provided code snippets:\n",
        "\n",
        "    Question: {question}\n",
        "\n",
        "    Code Repository Snippets:\n",
        "    {relevant_code}\n",
        "\n",
        "    Answer:\n",
        "    \"\"\")\n",
        "\n",
        "    llm_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
        "    response = llm_chain.run({\"question\": question, \"relevant_code\": relevant_code})\n",
        "    return response\n",
        "\n",
        "\n",
        "def analyze_code_best_practices(code_files):\n",
        "    \"\"\"Analyzes best practices, issues, and warnings in the repository.\"\"\"\n",
        "    repo_analysis = []\n",
        "    for file, code in code_files.items():\n",
        "        print(f\"Analyzing best practices for {file}...\")\n",
        "        prompt_template = PromptTemplate.from_template(\"\"\"\n",
        "        Review the following code snippet from {file} and provide best practices, potential issues, and warnings:\n",
        "\n",
        "        Code:\n",
        "        {code}\n",
        "\n",
        "        Best Practices:\n",
        "        -\n",
        "\n",
        "        Issues & Warnings:\n",
        "        -\n",
        "        \"\"\")\n",
        "\n",
        "        llm_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
        "        response = llm_chain.run({\"file\": file, \"code\": code[:2000]})  # Limit input size\n",
        "        repo_analysis.append(f\"\"\"📄 **{file}**:\n",
        "{response}\n",
        "\"\"\")\n",
        "\n",
        "    return \"\\n\".join(repo_analysis)\n",
        "\n",
        "\n",
        "def interactive_qa_system():\n",
        "    \"\"\"Runs an interactive Q&A system for the repository.\"\"\"\n",
        "    code_files = read_code_files(LOCAL_REPO_PATH)\n",
        "    print(\"\\n📢 Repository Q&A System Initialized! Type 'exit' to quit.\\n\")\n",
        "    while True:\n",
        "        question = input(\"Ask a question about the code repository: \")\n",
        "        if question.lower() == 'exit':\n",
        "            break\n",
        "        response = answer_question_about_code(question, code_files)\n",
        "        print(\"\\n🤖 AI Response:\\n\", response, \"\\n\")\n",
        "\n",
        "# Run best practices analysis\n",
        "if __name__ == \"__main__\":\n",
        "    code_files = read_code_files(LOCAL_REPO_PATH)\n",
        "    best_practices_report = analyze_code_best_practices(code_files)\n",
        "    print(\"\\n🛠 Best Practices & Issues Report:\\n\", best_practices_report)\n",
        "\n",
        "    # Save report\n",
        "    with open(\"/content/drive/MyDrive/Colab_Notebooks/best_practices_report.txt\", \"w\") as f:\n",
        "        f.write(best_practices_report)\n",
        "\n",
        "    # Start Q&A System\n",
        "    interactive_qa_system()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97P5OWc20IzT",
        "outputId": "f537ace6-687d-434a-bd74-f53e84d691f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analyzing best practices for /content/drive/MyDrive/Colab_Notebook/Robi_Datathon_0100_pandas/shahnawaz/utils/Robi_Datathon_problems_solution.ipynb...\n",
            "\n",
            "🛠 Best Practices & Issues Report:\n",
            " 📄 **/content/drive/MyDrive/Colab_Notebook/Robi_Datathon_0100_pandas/shahnawaz/utils/Robi_Datathon_problems_solution.ipynb**:\n",
            "```python\n",
            "#Problem 1\n",
            "import pandas as pd\n",
            "\n",
            "# Load datasets using pathlib for more robust path handling\n",
            "from pathlib import Path\n",
            "data_dir = Path(\"/content/drive/MyDrive/Colab Notebooks/\")  # Define data directory\n",
            "\n",
            "try:\n",
            "    purchase_data = pd.read_csv(data_dir / \"purchase.csv\")\n",
            "    boxes_data = pd.read_csv(data_dir / \"boxes.csv\")\n",
            "    problem_data = pd.read_csv(data_dir / \"problem1.csv\")\n",
            "except FileNotFoundError:\n",
            "    print(\"Error: One or more data files not found. Check the path.\")\n",
            "    # Handle the error appropriately, e.g., exit the script\n",
            "\n",
            "\n",
            "# Merge purchase and boxes datasets using a more informative merge method\n",
            "merged_data = pd.merge(purchase_data, boxes_data, on=\"BOX_ID\", how=\"left\") # Explicitly state the merge type\n",
            "\n",
            "\n",
            "# No need to reorder and rename if the order is already correct.  If reordering is necessary:\n",
            "# merged_data = merged_data[[\"PURCHASE_DATE\", \"MAGIC_KEY\", \"BOX_ID\", \"MILK\", \"MEAT\"]]\n",
            "\n",
            "# Predict purchases for Magic Keys in problem data using a more efficient approach\n",
            "problem_data[\"PURCHASE\"] = problem_data[\"MAGIC_KEY\"].isin(merged_data[\"MAGIC_KEY\"]) # Boolean is sufficient, no need for .astype(str)\n",
            "\n",
            "# Save submission file using pathlib\n",
            "submission_file = data_dir / \"submission_1.csv\"\n",
            "submission_df = problem_data[[\"MAGIC_KEY\", \"PURCHASE\"]]\n",
            "submission_df.to_csv(submission_file, index=False)\n",
            "\n",
            "\n",
            "\n",
            "# Problem 2 (Incomplete in original snippet) - Example of a more complete structure\n",
            "# ... (Code for problem 2 would go here) ...\n",
            "```\n",
            "\n",
            "**Best Practices:**\n",
            "\n",
            "* **Pathlib:** Use `pathlib` for file path management. It makes your code more robust and platform-independent, avoiding issues with different operating system path separators.\n",
            "* **Explicit Merge:** Specify the `how` argument in `pd.merge` (e.g., `how=\"left\"`). This makes the code clearer and prevents unexpected behavior if the default merge type changes.\n",
            "* **Error Handling:** Include error handling (e.g., `try...except`) to gracefully handle potential file not found errors.\n",
            "* **Boolean Columns:**  Don't convert boolean columns to strings unnecessarily. Pandas handles boolean values efficiently.\n",
            "* **Concise Code:** Avoid redundant column reordering/renaming if the desired order is already present after the merge.\n",
            "* **Comments:**  Keep comments clear, concise, and focused on explaining the *why*, not just the *what*.\n",
            "* **Data Validation:** After loading data, add checks (e.g., `df.head()`, `df.info()`, checks for expected columns) to ensure the data is as expected.  This helps catch errors early.\n",
            "\n",
            "\n",
            "**Issues & Warnings:**\n",
            "\n",
            "* **Hardcoded Paths:**  The original code used hardcoded paths, making it less portable. Pathlib addresses this.\n",
            "* **Implicit Merge:** The `pd.merge` without `how` relies on the default, which can be ambiguous.\n",
            "* **Unnecessary Type Conversion:** Converting the boolean `isin` result to a string is inefficient.\n",
            "* **Incomplete Problem 2:** The provided snippet for Problem 2 was incomplete. The revised code provides a structure for completing it.\n",
            "* **Missing Data Checks:** The code lacks any validation of the loaded data. This could lead to silent errors if the data is not in the expected format.\n",
            "* **No Comments Explaining Logic:**  Comments should explain the reasoning behind the code, not just what the code does. For example, *why* are the datasets being merged? What is the goal of Problem 1?\n",
            "\n",
            "\n",
            "By addressing these issues and incorporating the best practices, the code becomes more readable, robust, and maintainable.  Remember to add more specific error handling and data validation steps based on the expected characteristics of your data files.  Good error handling and data validation are crucial for any data processing task.\n",
            "\n",
            "\n",
            "📢 Repository Q&A System Initialized! Type 'exit' to quit.\n",
            "\n",
            "Ask a question about the code repository: What was the role of magic key here? \n",
            "\n",
            "🤖 AI Response:\n",
            " The `MAGIC_KEY` acts as the joining key between the `purchase` data (containing purchase information and box IDs) and the `problem1` data (containing magic keys for which purchases need to be predicted). It's used to determine whether a given `MAGIC_KEY` in `problem1.csv` has a corresponding purchase record in the merged dataset of `purchase.csv` and `boxes.csv`.  Essentially, it's used to link a customer (represented by `MAGIC_KEY`) to their purchases. \n",
            "\n",
            "Ask a question about the code repository: what was here primary key, secondary key and composite key?\n",
            "\n",
            "🤖 AI Response:\n",
            " The code doesn't explicitly define primary keys, secondary keys, or composite keys within the dataframes. It uses pandas to merge data based on shared columns, but it doesn't set or declare any key constraints.\n",
            "\n",
            "Here's a breakdown based on how the code *uses* the columns:\n",
            "\n",
            "* **`BOX_ID`:**  Acts like a primary key for the `boxes.csv` data and a foreign key in `purchase.csv`. The code merges `purchase.csv` and `boxes.csv` using `BOX_ID`.  This suggests `BOX_ID` uniquely identifies rows within the `boxes` data.\n",
            "\n",
            "* **`MAGIC_KEY`:**  Seems to act like a primary key for an unshown dataset and is used to check if keys from `problem1.csv` exist in the merged data.  While the code doesn't explicitly create a primary key constraint, it uses `MAGIC_KEY` to identify records for the purchasing prediction.\n",
            "\n",
            "* **Composite Key (Implied):** There's no explicit composite key defined. However, the combination of `PURCHASE_DATE` and `MAGIC_KEY` *might* function as a unique identifier for purchases within the `purchase.csv` data, but this is not guaranteed based solely on the provided code.  If a customer could make multiple purchases on the same day, then even this combination wouldn't be a true primary key.\n",
            "\n",
            "**Important Note:** The code reads the CSV files into pandas DataFrames.  Pandas itself does not enforce database-like primary, secondary, or composite key constraints. These concepts are relevant to relational databases.  While the code's logic implies the *intended use* of certain columns as keys, they are not formally defined as such within the pandas context.  If the data were loaded into a database, you would need to define these constraints using SQL. \n",
            "\n",
            "Ask a question about the code repository: what would you rate this codebase?\n",
            "\n",
            "🤖 AI Response:\n",
            " The code appears to be a straightforward data processing script using pandas, likely for a data competition or similar task.  It's not particularly complex, and there are some areas that could be improved.  I'd rate it as **fair** or **average**.\n",
            "\n",
            "Here's a breakdown of the positives and negatives:\n",
            "\n",
            "**Positives:**\n",
            "\n",
            "* **Functional:** The code achieves its goal of merging data, manipulating columns, and creating a submission file.\n",
            "* **Uses pandas:**  Leveraging pandas is a good choice for this type of data manipulation.\n",
            "* **Clear column renaming:** Explicitly renaming columns, even if to the same names, improves readability and ensures correctness.\n",
            "\n",
            "\n",
            "**Negatives:**\n",
            "\n",
            "* **Redundant column renaming:**  The code reorders columns and then immediately renames them to the same names. This is unnecessary.  The desired column order should be established directly after the merge.\n",
            "* **Hardcoded file paths:**  Using absolute paths like \"/content/drive/MyDrive/Colab Notebooks/...\" makes the code less portable and harder to run in different environments.  Relative paths or configuration files would be preferable.\n",
            "* **Lack of comments explaining logic:** While the code is somewhat self-explanatory, comments explaining the *why* behind certain operations (e.g., why specific columns are being selected) would improve maintainability and understanding.\n",
            "* **No error handling:**  The code doesn't include any error handling (e.g., `try-except` blocks) to handle potential issues like missing files or incorrect data formats.\n",
            "* **Inefficient prediction method:** Using `isin` and then converting to string for the \"PURCHASE\" column is less efficient than directly using a boolean.  It also introduces potential string comparison issues later if the data is used elsewhere.\n",
            "\n",
            "**Suggestions for Improvement:**\n",
            "\n",
            "* **Combine column reordering and renaming:**  `merged_data = pd.merge(purchase_data, boxes_data, on=\\\"BOX_ID\\\")[[\\\"PURCHASE_DATE\\\", \\\"MAGIC_KEY\\\", \\\"BOX_ID\\\", \\\"MILK\\\", \\\"MEAT\\\"]]`\n",
            "* **Use relative file paths or configuration files:**  This makes the code more portable.\n",
            "* **Add comments explaining the purpose of each step.**\n",
            "* **Implement error handling to gracefully handle potential issues.**\n",
            "* **Improve prediction method:**  `problem_data[\\\"PURCHASE\\\"] = problem_data[\\\"MAGIC_KEY\\\"].isin(merged_data[\\\"MAGIC_KEY\\\"])` This directly assigns a boolean value.\n",
            "\n",
            "\n",
            "By addressing these issues, the code could be significantly improved and would merit a higher rating. \n",
            "\n",
            "Ask a question about the code repository: quit\n",
            "\n",
            "🤖 AI Response:\n",
            " The provided code doesn't contain any explicit \"quit\" command or related logic.  It focuses on data processing using pandas.  Therefore, there's no information within the snippets to explain what \"quit\" refers to or how it would behave. \n",
            "\n"
          ]
        }
      ]
    }
  ]
}