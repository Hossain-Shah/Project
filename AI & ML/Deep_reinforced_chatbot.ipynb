{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yhs87Q-jotnr",
        "outputId": "5a55e400-bc86-4fe3-e2c1-72a44029bf85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Libraries import\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "import gym\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import wordnet\n",
        "from transformers import TFGPT2LMHeadModel"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Environment architecture\n",
        "class ChatbotEnv(gym.Env):\n",
        "    metadata = {'render.modes': ['human']}\n",
        "\n",
        "    def __init__(self, messages):\n",
        "        self.action_space = gym.spaces.Discrete(2) # Two possible actions: respond with a fixed message or ask a follow-up question\n",
        "        self.observation_space = gym.spaces.Box(low=0, high=1, shape=(len(messages),)) # Binary vector representing the user's message\n",
        "        self.messages = messages\n",
        "        self.state = None\n",
        "\n",
        "    def step(self, action):\n",
        "        if action == 0: # Respond with a fixed message line\n",
        "            response = \"Thank you for your message. Our team will get back to you soon.\"\n",
        "            reward = 1\n",
        "            done = True\n",
        "        else: # Ask a follow-up question from user\n",
        "            response = \"Can you provide more information about your request?\"\n",
        "            reward = 0\n",
        "            done = False\n",
        "\n",
        "        return self.state, reward, done, {'response': response}\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = np.zeros(len(self.messages))\n",
        "        return self.state\n",
        "\n",
        "class ChatbotAgent:\n",
        "    def __init__(self, env, model, tokenizer):\n",
        "        self.env = env\n",
        "        self.model = TFGPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "        self.tokenizer = Tokenizer()\n",
        "\n",
        "    def train(self, episodes):\n",
        "        max_steps_per_episode = 1\n",
        "        for episode in range(episodes):\n",
        "            # Reset the environment for a new episode\n",
        "            observation = self.env.reset()\n",
        "\n",
        "            # Implement your training logic here\n",
        "            # For example, loop through steps, choose actions, and update the model\n",
        "            for step in range(max_steps_per_episode):\n",
        "                action = self.select_action(observation)\n",
        "                next_observation, reward, done, info = self.env.step(action)\n",
        "                observation = next_observation\n",
        "\n",
        "                if done:\n",
        "                    break\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        # Use NLTK word_tokenize for tokenization\n",
        "        return word_tokenize(text)\n",
        "\n",
        "    def select_action(self, observation):\n",
        "        # Tokenize the elements of observation\n",
        "        observation = [self.tokenize(str(item)) for item in observation]\n",
        "\n",
        "        # Flatten the list of tokenized words into a single list\n",
        "        observation = [word for sublist in observation for word in sublist]\n",
        "        self.tokenizer.fit_on_texts(observation)\n",
        "\n",
        "        # Convert the observation (list of tokens) to a single string\n",
        "        input_text = ' '.join(observation)\n",
        "\n",
        "        # Tokenize the input text\n",
        "        input_sequence = self.tokenizer.texts_to_sequences([' '.join(observation)])[0]\n",
        "\n",
        "        # Pad the sequence to a fixed length\n",
        "        input_sequence = pad_sequences([input_sequence], maxlen=50, padding='post')\n",
        "        with tf.device(\"/gpu:0\"):  # Use GPU if available\n",
        "            output = self.model.generate(input_sequence, max_length=1050, num_beams=5, no_repeat_ngram_size=2, top_k=50, top_p=0.95)\n",
        "\n",
        "        # Decode the generated response\n",
        "        response = self.tokenizer.sequences_to_texts(output.numpy())[0]\n",
        "\n",
        "\n",
        "        # Decide on the action based on the generated response (you can define your own logic)\n",
        "        action = 0 if \"positive\" in response.lower() else 1\n",
        "\n",
        "        return action"
      ],
      "metadata": {
        "id": "HTzCs9wbpJdB"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this step will Collect and preprocess data\n",
        "messages = ['How can I help you?', 'Can you provide more information?', 'Thank you for your message.']\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Load the chat logs\n",
        "chat_logs = []\n",
        "with open('/content/drive/MyDrive/Colab_Notebooks/chat_log.txt', 'r') as f:\n",
        "    for line in f:\n",
        "        line = line.strip().lower()\n",
        "        if line:\n",
        "            chat_logs.append(line)\n",
        "\n",
        "# Tokenize and preprocess the chat messages\n",
        "tokenized_messages = []\n",
        "for message in messages:\n",
        "    tokens = word_tokenize(message.lower())\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
        "    tokenized_messages.append(' '.join(tokens))\n",
        "\n",
        "# Tokenize and preprocess chat logs through the below code.\n",
        "tokenized_logs = []\n",
        "for log in chat_logs:\n",
        "    tokens = word_tokenize(log)\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
        "    tokenized_logs.append(' '.join(tokens))\n",
        "\n",
        "# Create a tokenizer and fit it on tokenized messages.\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(tokenized_messages)\n",
        "\n",
        "# Convert tokenized log to sequences\n",
        "sequences = tokenizer.texts_to_sequences(tokenized_logs)\n",
        "\n",
        "# Pad sequences to ensure each sequences have the same length.\n",
        "max_len = max([len(seq) for seq in sequences])\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_len, padding='post')"
      ],
      "metadata": {
        "id": "PPD7JXpsrLXI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This step will create a chatbot environment and agent.\n",
        "model = TFGPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "env = ChatbotEnv(tokenized_messages)\n",
        "agent = ChatbotAgent(env, model, tokenizer)\n",
        "\n",
        "# Train the agent with the a train() function.\n",
        "agent.train(episodes=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_TISd_WGu1ns",
        "outputId": "c82b0a52-ecab-41bc-c038-74a21f8ebe59"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
            "\n",
            "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
            "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
            "\n",
            "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the created chatbot\n",
        "while True:\n",
        "    user_input1 = input(\"User: \")\n",
        "    tokens = word_tokenize(user_input1.lower())\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
        "    tokenized_input = ' '.join(tokens)\n",
        "    sequence = tokenizer.texts_to_sequences([tokenized_input])\n",
        "    padded_sequence = pad_sequences(sequence, maxlen=max_len, padding='post')\n",
        "    action = agent.select_action(padded_sequence[0])\n",
        "    response = env.step(action)[3]['response']\n",
        "    print(\"Chatbot : \" + response) # Test the chatbot\n",
        "\n",
        "while True:\n",
        "    user_input1 = input(\"User: \")\n",
        "    tokens = word_tokenize(user_input1.lower())\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
        "    tokenized_input = ' '.join(tokens)\n",
        "    sequence = tokenizer.texts_to_sequences([tokenized_input])\n",
        "    padded_sequence = pad_sequences(sequence, maxlen=max_len, padding='post')\n",
        "    action = agent.select_action(padded_sequence[0])\n",
        "    response = env.step(action)[3]['response']\n",
        "    print(\"Chatbot response: \" + response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EnOcYa7iHF5v",
        "outputId": "5acbd370-72a1-41c3-b7ab-84460e0ad34c"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User: What is chat?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chatbot : Can you provide more information about your request?\n",
            "User: online communication between two person\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chatbot : Can you provide more information about your request?\n",
            "User: no\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chatbot : Can you provide more information about your request?\n",
            "User: What is messenger?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chatbot : Can you provide more information about your request?\n",
            "User: Yes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chatbot : Can you provide more information about your request?\n",
            "User: No, Welcome\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chatbot : Can you provide more information about your request?\n"
          ]
        }
      ]
    }
  ]
}