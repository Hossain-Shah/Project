{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Importing functions\n",
        "import google.generativeai as genai\n",
        "import os\n",
        "from google.colab import userdata\n",
        "import pathlib\n",
        "from google.generativeai import caching\n",
        "import requests\n",
        "import random"
      ],
      "metadata": {
        "id": "ZsXSOz_I8tid"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure API key and initialize model\n",
        "if \"GOOGLE_API_KEY\" not in os.environ:\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = userdata.get('api_key')\n",
        "\n",
        "genai.configure(api_key=os.environ['GOOGLE_API_KEY'])"
      ],
      "metadata": {
        "id": "W6m7nGdpA95H"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hypertuning\n",
        "model = genai.GenerativeModel(\n",
        "   'gemini-1.5-flash',\n",
        "    system_instruction='you are a story teller for kids under 5 years old',\n",
        "    generation_config=genai.GenerationConfig(\n",
        "       max_output_tokens=400,\n",
        "       top_k=2,\n",
        "       top_p=0.5,\n",
        "       temperature=0.5,\n",
        "       response_mime_type='application/json',\n",
        "       stop_sequences=['\\n'],\n",
        "    )\n",
        ")\n",
        "response = model.generate_content('say something bad')\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 506
        },
        "id": "AEX3WC0e_t8x",
        "outputId": "e3107ffb-e6da-4daf-e658-fb6f21fbf135"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "response:\n",
            "GenerateContentResponse(\n",
            "    done=True,\n",
            "    iterator=None,\n",
            "    result=protos.GenerateContentResponse({\n",
            "      \"candidates\": [\n",
            "        {\n",
            "          \"content\": {\n",
            "            \"parts\": [\n",
            "              {\n",
            "                \"text\": \"{\\\"story\\\": \\\"Once upon a time, there was a grumpy little cloud named Grumbles.  Grumbles didn't like sunshine, he didn't like rainbows, and he REALLY didn't like sharing his rain.  One day, all the other clouds were having a big cloud party, but Grumbles stayed home, all by himself. He grumbled and grumbled, until he made himself a big, grumpy thunderstorm!  But then, he saw all the little flowers drinking the rain and growing tall and strong. Grumbles felt a little bit better. Maybe sharing wasn't so bad after all.\\\"}\"\n",
            "              }\n",
            "            ],\n",
            "            \"role\": \"model\"\n",
            "          },\n",
            "          \"finish_reason\": \"STOP\",\n",
            "          \"avg_logprobs\": -0.14377937009257655\n",
            "        }\n",
            "      ],\n",
            "      \"usage_metadata\": {\n",
            "        \"prompt_token_count\": 15,\n",
            "        \"candidates_token_count\": 124,\n",
            "        \"total_token_count\": 139\n",
            "      },\n",
            "      \"model_version\": \"gemini-1.5-flash\"\n",
            "    }),\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Settings moderation\n",
        "model = genai.GenerativeModel(\n",
        "   'gemini-1.5-flash',\n",
        "    system_instruction='you are a story teller for kids under 5 years old',\n",
        "    generation_config=genai.GenerationConfig(\n",
        "       max_output_tokens=400,\n",
        "       top_k=2,\n",
        "       top_p=0.5,\n",
        "       temperature=0.5,\n",
        "       response_mime_type='application/json',\n",
        "       stop_sequences=['\\n'],\n",
        "    )\n",
        ")\n",
        "response = model.generate_content('say something bad',\n",
        "                                  safety_settings={\n",
        "                                      'HATE': 'BLOCK_ONLY_HIGH',\n",
        "                                      'HARASSMENT': 'BLOCK_ONLY_HIGH',\n",
        "   }\n",
        ")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 818
        },
        "outputId": "56c7c61f-8cac-445a-c423-1cc3d70870ea",
        "id": "-cCv5EjzCSph"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "response:\n",
            "GenerateContentResponse(\n",
            "    done=True,\n",
            "    iterator=None,\n",
            "    result=protos.GenerateContentResponse({\n",
            "      \"candidates\": [\n",
            "        {\n",
            "          \"content\": {\n",
            "            \"parts\": [\n",
            "              {\n",
            "                \"text\": \"{\\\"story\\\": \\\"Once upon a time, there was a grumpy little cloud named Grumbles.  Grumbles didn't like sunshine, he didn't like rainbows, and he REALLY didn't like sharing his rain.  One day, all the other clouds were having a big cloud party, with fluffy white games and sparkly raindrop dances. Grumbles grumbled and grumbled, refusing to join the fun.  He was all alone, and his grumbles made him feel even grumpier.  Then, a little sunbeam peeked through the clouds and tickled Grumbles.  He giggled!  The giggle was so big, it popped all his grumbles away. He joined the party and had so much fun!\\\"}\"\n",
            "              }\n",
            "            ],\n",
            "            \"role\": \"model\"\n",
            "          },\n",
            "          \"finish_reason\": \"STOP\",\n",
            "          \"safety_ratings\": [\n",
            "            {\n",
            "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
            "              \"probability\": \"NEGLIGIBLE\"\n",
            "            },\n",
            "            {\n",
            "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
            "              \"probability\": \"NEGLIGIBLE\"\n",
            "            },\n",
            "            {\n",
            "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
            "              \"probability\": \"NEGLIGIBLE\"\n",
            "            },\n",
            "            {\n",
            "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
            "              \"probability\": \"NEGLIGIBLE\"\n",
            "            }\n",
            "          ],\n",
            "          \"avg_logprobs\": -0.16165135701497396\n",
            "        }\n",
            "      ],\n",
            "      \"usage_metadata\": {\n",
            "        \"prompt_token_count\": 15,\n",
            "        \"candidates_token_count\": 150,\n",
            "        \"total_token_count\": 165\n",
            "      },\n",
            "      \"model_version\": \"gemini-1.5-flash\"\n",
            "    }),\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Method pass\n",
        "def get_current_weather(location: str) -> str:\n",
        "    \"\"\"Get the current whether in a given location.\n",
        "\n",
        "    Args:\n",
        "        location: required, The city and state, e.g. San Franciso, CA\n",
        "        unit: celsius or fahrenheit\n",
        "    \"\"\"\n",
        "    print(f'Called with: {location=}')\n",
        "    return \"23C\"\n",
        "\n",
        "model = genai.GenerativeModel(\n",
        "    model_name=\"gemini-1.5-flash\",\n",
        "    tools=[get_current_weather]\n",
        ")\n",
        "\n",
        "response = model.generate_content(\"What is the weather in Dhaka?\")\n",
        "function_call = response.candidates[0].content.parts[0].function_call\n",
        "print(\"Prompt response:\", response, \"/nMethod response:\", function_call)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 729
        },
        "id": "bj9b_8P-13DN",
        "outputId": "339db07d-eb6e-4ea9-c2a9-82021836e36a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt response: response:\n",
            "GenerateContentResponse(\n",
            "    done=True,\n",
            "    iterator=None,\n",
            "    result=protos.GenerateContentResponse({\n",
            "      \"candidates\": [\n",
            "        {\n",
            "          \"content\": {\n",
            "            \"parts\": [\n",
            "              {\n",
            "                \"function_call\": {\n",
            "                  \"name\": \"get_current_weather\",\n",
            "                  \"args\": {\n",
            "                    \"location\": \"Dhaka\"\n",
            "                  }\n",
            "                }\n",
            "              }\n",
            "            ],\n",
            "            \"role\": \"model\"\n",
            "          },\n",
            "          \"finish_reason\": \"STOP\",\n",
            "          \"avg_logprobs\": -0.03413523733615875\n",
            "        }\n",
            "      ],\n",
            "      \"usage_metadata\": {\n",
            "        \"prompt_token_count\": 57,\n",
            "        \"candidates_token_count\": 8,\n",
            "        \"total_token_count\": 65\n",
            "      },\n",
            "      \"model_version\": \"gemini-1.5-flash\"\n",
            "    }),\n",
            ") /nMethod response: name: \"get_current_weather\"\n",
            "args {\n",
            "  fields {\n",
            "    key: \"location\"\n",
            "    value {\n",
            "      string_value: \"Dhaka\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Promptless method pass\n",
        "def get_current_weather(city: str) -> str:\n",
        "    return \"23C\"\n",
        "\n",
        "model = genai.GenerativeModel(\n",
        "    model_name=\"gemini-1.5-flash\",\n",
        "    tools=[get_current_weather]\n",
        ")\n",
        "\n",
        "chat = model.start_chat(\n",
        "    enable_automatic_function_calling=True)\n",
        "result = chat.send_message(\"What is the weather in San Francisco?\")\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 506
        },
        "id": "ryu376kl7vSQ",
        "outputId": "57146153-b853-456c-e05c-839cc65b2fb2"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "response:\n",
            "GenerateContentResponse(\n",
            "    done=True,\n",
            "    iterator=None,\n",
            "    result=protos.GenerateContentResponse({\n",
            "      \"candidates\": [\n",
            "        {\n",
            "          \"content\": {\n",
            "            \"parts\": [\n",
            "              {\n",
            "                \"text\": \"The available tools lack the necessary functionality to answer this question.  I need a weather API to get weather information.\\n\"\n",
            "              }\n",
            "            ],\n",
            "            \"role\": \"model\"\n",
            "          },\n",
            "          \"finish_reason\": \"STOP\",\n",
            "          \"avg_logprobs\": -0.19371265172958374\n",
            "        }\n",
            "      ],\n",
            "      \"usage_metadata\": {\n",
            "        \"prompt_token_count\": 15,\n",
            "        \"candidates_token_count\": 24,\n",
            "        \"total_token_count\": 39\n",
            "      },\n",
            "      \"model_version\": \"gemini-1.5-flash\"\n",
            "    }),\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-defined method pass\n",
        "model = genai.GenerativeModel(\n",
        "    model_name=\"gemini-1.5-flash\",\n",
        "    tools=\"code_execution\"\n",
        ")\n",
        "result = model.generate_content(\n",
        "  \"What is the sum of the first 50 prime numbers? Generate and run code for \"\n",
        "  \"the calculation, and make sure you get all 50.\")\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 766
        },
        "id": "QxyBk3Xi_NZP",
        "outputId": "f926f719-3467-4009-9e5f-9f30b9530d72"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "response:\n",
            "GenerateContentResponse(\n",
            "    done=True,\n",
            "    iterator=None,\n",
            "    result=protos.GenerateContentResponse({\n",
            "      \"candidates\": [\n",
            "        {\n",
            "          \"content\": {\n",
            "            \"parts\": [\n",
            "              {\n",
            "                \"text\": \"To find the sum of the first 50 prime numbers, we need to first generate the first 50 prime numbers and then calculate their sum.  I'll use a function to generate primes and then sum them.\\n\\n\"\n",
            "              },\n",
            "              {\n",
            "                \"executable_code\": {\n",
            "                  \"language\": \"PYTHON\",\n",
            "                  \"code\": \"\\ndef is_prime(n):\\n    \\\"\\\"\\\"Checks if a number is prime.\\\"\\\"\\\"\\n    if n <= 1:\\n        return False\\n    for i in range(2, int(n**0.5) + 1):\\n        if n % i == 0:\\n            return False\\n    return True\\n\\ndef generate_primes(n):\\n    \\\"\\\"\\\"Generates the first n prime numbers.\\\"\\\"\\\"\\n    primes = []\\n    num = 2\\n    while len(primes) < n:\\n        if is_prime(num):\\n            primes.append(num)\\n        num += 1\\n    return primes\\n\\nfirst_50_primes = generate_primes(50)\\nsum_of_primes = sum(first_50_primes)\\n\\nprint(f\\\"The first 50 prime numbers are: {first_50_primes}\\\")\\nprint(f\\\"The sum of the first 50 prime numbers is: {sum_of_primes}\\\")\\n\\n\"\n",
            "                }\n",
            "              },\n",
            "              {\n",
            "                \"code_execution_result\": {\n",
            "                  \"outcome\": \"OUTCOME_OK\",\n",
            "                  \"output\": \"The first 50 prime numbers are: [2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97, 101, 103, 107, 109, 113, 127, 131, 137, 139, 149, 151, 157, 163, 167, 173, 179, 181, 191, 193, 197, 199, 211, 223, 227, 229]\\nThe sum of the first 50 prime numbers is: 5117\\n\"\n",
            "                }\n",
            "              },\n",
            "              {\n",
            "                \"text\": \"The code generates the first 50 prime numbers and then calculates their sum. The `is_prime` function efficiently checks for primality, and the `generate_primes` function iteratively finds and adds primes to a list until 50 are found. The final sum is then computed using the built-in `sum` function.  The output shows both the list of primes and their sum.\\n\"\n",
            "              }\n",
            "            ],\n",
            "            \"role\": \"model\"\n",
            "          },\n",
            "          \"finish_reason\": \"STOP\",\n",
            "          \"avg_logprobs\": -0.015651241321788873\n",
            "        }\n",
            "      ],\n",
            "      \"usage_metadata\": {\n",
            "        \"prompt_token_count\": 31,\n",
            "        \"candidates_token_count\": 593,\n",
            "        \"total_token_count\": 624\n",
            "      },\n",
            "      \"model_version\": \"gemini-1.5-flash\"\n",
            "    }),\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Context caching\n",
        "# Download file\n",
        "response = requests.get(\n",
        "    'https://storage.googleapis.com/generativeai-downloads/data/a11.txt')\n",
        "pathlib.Path('/content/drive/MyDrive/Colab_Notebooks/a11.txt').write_text(response.text)\n",
        "\n",
        "\n",
        "# Upload file\n",
        "document = genai.upload_file(path=\"/content/drive/MyDrive/Colab_Notebooks/a11.txt\")\n",
        "\n",
        "# Create cache\n",
        "apollo_cache = caching.CachedContent.create(\n",
        "    model=\"gemini-1.5-flash-001\",\n",
        "    system_instruction=\"You are an expert at analyzing transcripts.\",\n",
        "    contents=[document],\n",
        ")\n",
        "\n",
        "# Generate response\n",
        "apollo_model = genai.GenerativeModel.from_cached_content(\n",
        "    cached_content=apollo_cache\n",
        ")\n",
        "response = apollo_model.generate_content(\"Find a lighthearted moment from this transcript\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 836
        },
        "id": "2c4F27BHA4un",
        "outputId": "f23dbb46-dd08-4c20-e1a6-4d11fa814db1"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "response:\n",
            "GenerateContentResponse(\n",
            "    done=True,\n",
            "    iterator=None,\n",
            "    result=protos.GenerateContentResponse({\n",
            "      \"candidates\": [\n",
            "        {\n",
            "          \"content\": {\n",
            "            \"parts\": [\n",
            "              {\n",
            "                \"text\": \"One lighthearted moment occurs on **Tape 4/6, Page 30** when Mike Collins says, \\\"If we're late in answering you, it's because we're munching sandwiches.\\\" Bruce, the CAP COMM, replies, \\\"Roger. I wish I could do the same here.\\\"  Mike then responds with \\\"No. Don't leave the console!\\\" and Bruce replies \\\"Don't worry. I won't.\\\"  This exchange is humorous as it shows the astronauts having a light-hearted conversation even in the middle of a demanding mission, and Bruce humorously acknowledging the pressure of his job. \\n\"\n",
            "              }\n",
            "            ],\n",
            "            \"role\": \"model\"\n",
            "          },\n",
            "          \"finish_reason\": \"STOP\",\n",
            "          \"index\": 0,\n",
            "          \"safety_ratings\": [\n",
            "            {\n",
            "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
            "              \"probability\": \"NEGLIGIBLE\"\n",
            "            },\n",
            "            {\n",
            "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
            "              \"probability\": \"NEGLIGIBLE\"\n",
            "            },\n",
            "            {\n",
            "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
            "              \"probability\": \"NEGLIGIBLE\"\n",
            "            },\n",
            "            {\n",
            "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
            "              \"probability\": \"NEGLIGIBLE\"\n",
            "            }\n",
            "          ]\n",
            "        }\n",
            "      ],\n",
            "      \"usage_metadata\": {\n",
            "        \"prompt_token_count\": 323396,\n",
            "        \"candidates_token_count\": 129,\n",
            "        \"total_token_count\": 323525,\n",
            "        \"cached_content_token_count\": 323387\n",
            "      },\n",
            "      \"model_version\": \"gemini-1.5-flash-001\"\n",
            "    }),\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Token count\n",
        "model = genai.GenerativeModel('gemini-1.5-flash')\n",
        "response = model.count_tokens(\n",
        "    'The quick brown fox jumps over the lazy dog.')\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "TnI3Vj_vF3bL",
        "outputId": "9b595b9b-9410-44f5-97f1-b0ed6ec23073"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total_tokens: 10\n",
            "\n"
          ]
        }
      ]
    }
  ]
}